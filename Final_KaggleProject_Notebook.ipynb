{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FDL_KaggleProject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "zhbcQIlD2xml"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connection to google drive for data\n",
        "from google.colab import drive\n",
        "drive = drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NebqlbcX7fby",
        "outputId": "2f7a3dac-c9fc-4e79-f69b-173b48818516"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install split-folders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u51YuiMFabYs",
        "outputId": "8a32a22f-4a69-4d8c-c351-45e8e2e6df57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting split-folders\n",
            "  Downloading split_folders-0.4.3-py3-none-any.whl (7.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the train images and masks into training and validation set \n",
        "import splitfolders\n",
        "splitfolders.fixed(\"/content/drive/MyDrive/FoundationDeepLearning_Labs/Kaggle_Project/Input\", \"/content/drive/MyDrive/FoundationDeepLearning_Labs/Kaggle_Project/Input_Split\", \n",
        "                   seed = 1337, fixed = (30), oversample = False, group_prefix = None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-cU8iqdaqYY",
        "outputId": "f63dae1c-2f87-424e-b38f-13a91e9dd3c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 522 files [00:57,  9.08 files/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preparation "
      ],
      "metadata": {
        "id": "1mtOWfKW3WiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directories in google drive where images are present.\n",
        "train_images_dir = '/content/drive/MyDrive/FoundationDeepLearning_Labs/Kaggle_Project/Input/train_images/'\n",
        "train_masks_dir = '/content/drive/MyDrive/FoundationDeepLearning_Labs/Kaggle_Project/Input/train_masks'\n",
        "#val_images_dir = '/content/drive/MyDrive/FoundationDeepLearning_Labs/Kaggle_Project/Input_Split/val/train_images'\n",
        "#val_masks_dir = '/content/drive/MyDrive/FoundationDeepLearning_Labs/Kaggle_Project/Input_Split/val/train_masks'\n",
        "test_images_dir = '/content/drive/MyDrive/FoundationDeepLearning_Labs/Kaggle_Project/test_images/test_images/'"
      ],
      "metadata": {
        "id": "6kiyiOXh7g3n"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for Train Dataset Preparation - \n",
        "class KaggleDataset(Dataset):\n",
        "  def __init__(self, image_dir, mask_dir, transform = None):\n",
        "    self.image_dir = image_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "    self.images = os.listdir(image_dir)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img_path = os.path.join(self.image_dir, self.images[index])\n",
        "    mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \".png\"))\n",
        "    image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "    mask = np.array(Image.open(mask_path).convert(\"L\"), dtype = np.float32)\n",
        "\n",
        "    if self.transform is not None:\n",
        "      augmentations = self.transform(image=image, mask = mask)\n",
        "      image = augmentations[\"image\"]\n",
        "      mask = augmentations[\"mask\"]\n",
        "\n",
        "    return image, mask"
      ],
      "metadata": {
        "id": "GF06ykuN5t4r"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Class for Test Dataset Preparation - \n",
        "class KaggleTestDataset(Dataset):\n",
        "  def __init__(self, image_dir, transform = None):\n",
        "    self.image_dir = image_dir\n",
        "    self.transform = transform\n",
        "    self.images = os.listdir(image_dir)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img_path = os.path.join(self.image_dir, self.images[index])\n",
        "    image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "\n",
        "    if self.transform is not None:\n",
        "      augmentations = self.transform(image=image)\n",
        "      image = augmentations[\"image\"]\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "-iATB5nB-8nb"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the latest version of Albumentations for data transformation - \n",
        "!pip install albumentations==0.4.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwHlc-qK9dDc",
        "outputId": "b09c206f-706f-40f9-b337-746453cfc91f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting albumentations==0.4.6\n",
            "  Downloading albumentations-0.4.6.tar.gz (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Collecting imgaug>=0.4.0\n",
            "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "\u001b[K     |████████████████████████████████| 948 kB 55.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.8.0)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.2.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2021.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65174 sha256=e735882381e4e56bcfc87d53c753ee91544b9a953b494ff3ec824e7672355eca\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/34/0f/cb2a5f93561a181a4bcc84847ad6aaceea8b5a3127469616cc\n",
            "Successfully built albumentations\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Attempting uninstall: imgaug\n",
            "    Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.6 imgaug-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations on train and test dataset - \n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from torchvision import transforms\n",
        "\n",
        "Image_height = 128\n",
        "Image_width = 128\n",
        "\n",
        "transforms = A.Compose(\n",
        "    [\n",
        "     A.Resize(height=Image_height, width=Image_width),\n",
        "     #A.Rotate(limit=35, p=1.0),\n",
        "     #A.HorizontalFlip(p=0.5),\n",
        "     #A.VerticalFlip(p=0.1),\n",
        "    #  A.Normalize(\n",
        "    #      mean=[0.0, 0.0, 0.0],\n",
        "    #      std = [1.0, 1.0, 1.0],\n",
        "    #      max_pixel_value = 255.0,\n",
        "    # ),\n",
        "     ToTensorV2(),\n",
        "    ],\n",
        ")\n",
        "\n",
        "test_transform = A.Compose(\n",
        "    [\n",
        "     A.Resize(height=Image_height, width = Image_width),\n",
        "    #  A.Normalize(\n",
        "    #      mean=[0.0, 0.0, 0.0],\n",
        "    #      std = [1.0, 1.0, 1.0],\n",
        "    #      max_pixel_value = 255.0\n",
        "    #  ),\n",
        "     ToTensorV2(),\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "3FORdav4Akoz"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train, Validation and Test Dataloader for model training and evaluation - \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_ds = KaggleDataset(image_dir=train_images_dir, mask_dir = train_masks_dir, transform=transforms)\n",
        "train_dataloader = DataLoader(train_ds, batch_size= 16, num_workers= 2, shuffle=True)\n",
        "\n",
        "#val_ds = KaggleDataset(image_dir=val_images_dir, mask_dir = val_masks_dir, transform=transforms)\n",
        "#val_dataloader = DataLoader(train_ds, batch_size= 16, num_workers= 2, shuffle=False)\n",
        "\n",
        "test_ds = KaggleTestDataset(image_dir=test_images_dir, transform=test_transform)\n",
        "test_dataloader = DataLoader(test_ds, batch_size=1, num_workers = 2, shuffle=False) #Keeping the batch size of 1 for test images.\n"
      ],
      "metadata": {
        "id": "x0-lTMef8fzW"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check on the dimensions of batch in train_dataloader\n",
        "image, mask = next(iter(train_dataloader))\n",
        "print(image.shape)\n",
        "print(mask.shape)"
      ],
      "metadata": {
        "id": "w6N6K82HAdbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "220a097d-ba60-42db-cdc2-ca6a778d86f3"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3, 128, 128])\n",
            "torch.Size([16, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Check on the dimensions of batch in val_dataloader\n",
        "# image, mask = next(iter(val_dataloader))\n",
        "# print(image.shape)\n",
        "# print(mask.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ySueXrKmKAX",
        "outputId": "82158940-5aad-4ef5-b2ef-ec0726f53610"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 3, 256, 256])\n",
            "torch.Size([8, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check on the dimensions of batch in test_dataloader\n",
        "image = next(iter(test_dataloader))\n",
        "print(image.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYMgAsyOA8AY",
        "outputId": "fbbbf664-3b49-489b-ba6c-7de47b80b140"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNET Model Architecture - "
      ],
      "metadata": {
        "id": "da0R2lKz3Kmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch import autograd\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self, in_ch, out_ch):\n",
        "    super(DoubleConv, self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "        nn.BatchNorm2d(out_ch),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "        nn.BatchNorm2d(out_ch),\n",
        "        nn.ReLU(inplace = True)\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "    return self.conv(input)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self, in_ch, out_ch):\n",
        "    super(UNet, self).__init__()\n",
        "\n",
        "    self.conv1 = DoubleConv(in_ch, 64)\n",
        "    self.pool1 = nn.MaxPool2d(2)\n",
        "    self.conv2 = DoubleConv(64, 128)\n",
        "    self.pool2 = nn.MaxPool2d(2)\n",
        "    self.conv3 = DoubleConv(128, 256)\n",
        "    self.pool3 = nn.MaxPool2d(2)\n",
        "    self.conv4 = DoubleConv(256, 512)\n",
        "    self.pool4 = nn.MaxPool2d(2)\n",
        "    self.conv5 = DoubleConv(512, 1024)\n",
        "\n",
        "    self.up6 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
        "    self.conv6 = DoubleConv(1024, 512)\n",
        "    self.up7 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
        "    self.conv7 = DoubleConv(512, 256)\n",
        "    self.up8 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "    self.conv8 = DoubleConv(256, 128)\n",
        "    self.up9 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "    self.conv9 = DoubleConv(128, 64)\n",
        "    self.conv10 = nn.Conv2d(64, out_ch, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    c1 = self.conv1(x)\n",
        "    p1 = self.pool1(c1)\n",
        "    c2 = self.conv2(p1)\n",
        "    p2 = self.pool2(c2)\n",
        "    c3 = self.conv3(p2)\n",
        "    p3 = self.pool3(c3)\n",
        "    c4 = self.conv4(p3)\n",
        "    p4 = self.pool4(c4)\n",
        "    c5 = self.conv5(p4)\n",
        "\n",
        "    up_6 = self.up6(c5)\n",
        "    merge6 = torch.cat([up_6, c4], dim=1)\n",
        "    c6 = self.conv6(merge6)\n",
        "    up_7 = self.up7(c6)\n",
        "    merge_7 = torch.cat([up_7, c3], dim=1)\n",
        "    c7 = self.conv7(merge_7)\n",
        "    up_8 = self.up8(c7)\n",
        "    merge8 = torch.cat([up_8, c2], dim=1)\n",
        "    c8 = self.conv8(merge8)\n",
        "    up_9 = self.up9(c8)\n",
        "    merge9 = torch.cat([up_9, c1], dim=1)\n",
        "    c9 = self.conv9(merge9)\n",
        "    c10 = self.conv10(c9)\n",
        "    return c10\n",
        "    "
      ],
      "metadata": {
        "id": "YlITVuLzz7XB"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the UNet"
      ],
      "metadata": {
        "id": "pXrj6WaI3ntX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check on the device\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using device:', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Mnoy7bp5DOa",
        "outputId": "91ff1c25-b25c-47cd-cbd9-1e531e25419a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Parameters - \n",
        "lr = 0.0001\n",
        "model = UNet(in_ch=3, out_ch=25).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "epochs = 40"
      ],
      "metadata": {
        "id": "FOEM7bc33lrc"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training and validation using above defined parameters - \n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "save_folder = '/content/drive/MyDrive/FoundationDeepLearning_Labs/Kaggle_Project/Trained_Models_Itr2/'\n",
        "\n",
        "total_train_loss = []\n",
        "total_val_loss = []\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "\n",
        "  #Training\n",
        "  model.train()\n",
        "  train_loss = []\n",
        "  #confusion_matrix.reset()\n",
        "\n",
        "  for i, batch, in enumerate(tqdm(train_dataloader)):\n",
        "    img_batch, lbl_batch = batch\n",
        "    img_batch, lbl_batch = img_batch.to(device), lbl_batch.type(torch.LongTensor).to(device)\n",
        "  \n",
        "    optimizer.zero_grad()\n",
        "    output = model(img_batch.float())\n",
        "    loss = criterion(output, lbl_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss.append(loss.item())\n",
        "    \n",
        "\n",
        "    if i%50 == 0:\n",
        "      print('Train (epoch {}/{}) [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, epochs, i, len(train_dataloader), 100.*i/len(train_dataloader), loss.item()\n",
        "      ))\n",
        "\n",
        "  train_loss_mean = np.mean(train_loss)\n",
        "  total_train_loss.append(train_loss_mean)\n",
        "  \n",
        "\n",
        "  #Validation \n",
        "  # model.eval()\n",
        "  # val_loss = []\n",
        "\n",
        "  # for i, batch, in enumerate(tqdm(val_dataloader)):\n",
        "  #   img_batch, lbl_batch = batch\n",
        "  #   img_batch, lbl_batch = img_batch.to(device), lbl_batch.type(torch.LongTensor).to(device)\n",
        "\n",
        "  #   output = model(img_batch.float())\n",
        "  #   loss = criterion(output, lbl_batch)\n",
        "\n",
        "  #   val_loss.append(loss.item())\n",
        "\n",
        "  # val_loss_mean = np.mean(val_loss)\n",
        "  # total_val_loss.append(val_loss_mean)\n",
        "  \n",
        "\n",
        "  print('Train_Loss: ', '%.3f' % train_loss_mean)\n",
        "  #print('Val_Loss: ', '%.3f' % val_loss_mean)\n",
        "\n",
        "  torch.save(model.state_dict(), save_folder + 'model_{}.pt'.format(epoch))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3tBOylF3cjU",
        "outputId": "7e978fff-39c9-482d-c482-22b399d2aef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 1/17 [00:08<02:11,  8.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (epoch 1/40) [0/17 (0%)]\tLoss: 3.290451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [01:03<00:00,  3.71s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_Loss:  2.910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 1/17 [00:08<02:12,  8.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (epoch 2/40) [0/17 (0%)]\tLoss: 2.593616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [01:03<00:00,  3.71s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_Loss:  2.391\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 1/17 [00:08<02:13,  8.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (epoch 3/40) [0/17 (0%)]\tLoss: 2.230849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [01:02<00:00,  3.68s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_Loss:  2.166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 1/17 [00:07<02:06,  7.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (epoch 4/40) [0/17 (0%)]\tLoss: 2.061323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [01:02<00:00,  3.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_Loss:  2.038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 1/17 [00:08<02:11,  8.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (epoch 5/40) [0/17 (0%)]\tLoss: 1.877487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [01:02<00:00,  3.68s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_Loss:  1.953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 1/17 [00:08<02:09,  8.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (epoch 6/40) [0/17 (0%)]\tLoss: 1.913936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [01:03<00:00,  3.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_Loss:  1.825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 1/17 [00:08<02:12,  8.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (epoch 7/40) [0/17 (0%)]\tLoss: 1.770752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [01:02<00:00,  3.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_Loss:  1.755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 1/17 [00:08<02:09,  8.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (epoch 8/40) [0/17 (0%)]\tLoss: 1.737635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [01:02<00:00,  3.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_Loss:  1.677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 1/17 [00:08<02:08,  8.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (epoch 9/40) [0/17 (0%)]\tLoss: 1.591150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [01:02<00:00,  3.68s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_Loss:  1.635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 1/17 [00:08<02:12,  8.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (epoch 10/40) [0/17 (0%)]\tLoss: 1.638770\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [01:03<00:00,  3.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_Loss:  1.577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 1/17 [00:08<02:15,  8.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (epoch 11/40) [0/17 (0%)]\tLoss: 1.519755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [01:02<00:00,  3.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_Loss:  1.527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 1/17 [00:08<02:13,  8.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (epoch 12/40) [0/17 (0%)]\tLoss: 1.418302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [01:02<00:00,  3.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_Loss:  1.465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 1/17 [00:08<02:13,  8.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (epoch 13/40) [0/17 (0%)]\tLoss: 1.396742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [01:03<00:00,  3.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_Loss:  1.398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 1/17 [00:08<02:11,  8.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (epoch 14/40) [0/17 (0%)]\tLoss: 1.329883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [01:02<00:00,  3.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_Loss:  1.364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 1/17 [00:08<02:10,  8.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (epoch 15/40) [0/17 (0%)]\tLoss: 1.282211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 53%|█████▎    | 9/17 [00:39<00:36,  4.56s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and Validation Loss Curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(list(range(epochs+1))[1:], total_train_loss)\n",
        "#plt.plot(list(range(epochs+1))[1:], total_val_loss)\n",
        "#plt.legend(['train','val'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "y8IrSV60tDeM",
        "outputId": "e0f17846-f7b3-4b8a-ef19-1647b8873cb2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output on Test Images"
      ],
      "metadata": {
        "id": "DWAX2AUrBtWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#List of the names of images from test dataset - \n",
        "\n",
        "import pandas as pd\n",
        "def create_df():\n",
        "    name = []\n",
        "    for dirname, _, filenames in os.walk(test_images_dir):\n",
        "        for filename in filenames:\n",
        "            name.append(filename.split('.')[0])\n",
        "    \n",
        "    return pd.DataFrame({'id': name}, index = np.arange(0, len(name)))\n",
        "\n",
        "df_test = create_df()\n",
        "print('Total Images: ', len(df_test))\n",
        "\n",
        "test = df_test['id'].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1k88hyR4cdi",
        "outputId": "402df549-d18c-4d3b-a702-878421c548c6"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Images:  112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the most optimal epoch on test images for mask output\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "model = UNet(3, 27)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/FoundationDeepLearning_Labs/Kaggle_Project/Trained_Models_Itr2/model_29.pt'))\n",
        "\n",
        "# Directory for saving the output masks\n",
        "save_path = '/content/drive/MyDrive/FoundationDeepLearning_Labs/Kaggle_Project/test_preds_itr2/'\n",
        "\n",
        "model.eval()\n",
        "\n",
        "i = 0\n",
        "with torch.no_grad():\n",
        "  for x in test_dataloader:\n",
        "    y = model(x.float())\n",
        "\n",
        "    y = F.softmax(y, dim=1) # Convert the network output values to probabilistic array [1x25x128x128]\n",
        "    y = torch.argmax(y, dim=1) # take the index corresponding to larger value [1x1x128x128]\n",
        "    \n",
        "    y = y.cpu().squeeze(0)\n",
        "\n",
        "    mask = y.detach().numpy().copy()\n",
        "\n",
        "    resize = A.Resize(4000, 3000, interpolation=cv2.INTER_NEAREST)\n",
        "    aug = resize(image = mask)\n",
        "    mask = aug['image']\n",
        "\n",
        "    # Save the mask image as .png file\n",
        "    cv2.imwrite(save_path + str(test[i]) + '.png', mask)\n",
        "    \n",
        "    i = i+1"
      ],
      "metadata": {
        "id": "dN_FwXtB8MHs"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission file using the prepare_submssion.py code"
      ],
      "metadata": {
        "id": "MjGPuiJMFjsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def rle_encode(img):\n",
        "    '''\n",
        "    img: numpy array, 1 - mask, 0 - background\n",
        "    Returns run length as string formated\n",
        "    '''\n",
        "    pixels = img.flatten()\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    return ' '.join(str(x) for x in runs)\n",
        "\n",
        "def rle_decode(mask_rle, shape):\n",
        "    '''\n",
        "    mask_rle: run-length as string formated (start length)\n",
        "    shape: (height,width) of array to return\n",
        "    Returns numpy array, 1 - mask, 0 - background\n",
        "    '''\n",
        "    s = mask_rle.split()\n",
        "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
        "    starts -= 1\n",
        "    ends = starts + lengths\n",
        "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
        "    for lo, hi in zip(starts, ends):\n",
        "        img[lo:hi] = 1\n",
        "    return img.reshape(shape)\n",
        "\n",
        "\n",
        "def create_rles():\n",
        "    \"\"\"Used for Kaggle submission: predicts and encode all test images\"\"\"\n",
        "    dir = save_path\n",
        "    N = len(list(os.listdir(dir)))\n",
        "    with open('submission_file.csv', 'w') as f: \n",
        "        f.write('ImageClassId,rle_mask\\n')\n",
        "        for index, i in enumerate(os.listdir(dir)):\n",
        "            # print('{}/{}'.format(index, N))\n",
        "\n",
        "            mask = Image.open(dir + i)\n",
        "            mask = mask.resize((1024, 1024), resample=Image.NEAREST) #Image.BICUBIC\n",
        "            mask = np.array(mask)\n",
        "\n",
        "            for x in range(1, 25):\n",
        "                enc = rle_encode(mask == x)\n",
        "                f.write(f\"{i.split('_')[0]}_{x},{enc}\\n\")\n",
        "\n",
        "create_rles()"
      ],
      "metadata": {
        "id": "0LSEbLOfFix7"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zGOeUg9iiipP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}